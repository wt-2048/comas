import srsly
import random
import os
import itertools
from abc import ABC
import math
from typing import Callable, Dict, List, Optional
from collections import defaultdict

import torch
from tqdm import tqdm
from datasets import Dataset, load_dataset
import ray
import torch
from dataclasses import dataclass
from ray.util.placement_group import placement_group

from marti.trainers.ppo.actor import ActorModelRayActor
from marti.trainers.ppo.critic import CriticModelRayActor
from marti.models.vllm.engine import create_vllm_engines
from marti.models.ray_launcher import PPORayActorGroup, ReferenceModelRayActor, RewardModelRayActor
from marti.data.prompts_loader import PromptDatasetWithLabel
from marti.data.sft_loader import SFTDataset
from marti.helpers.common import blending_datasets, get_tokenizer
from marti.helpers.distributed.distributed_sampler import DistributedSampler, ResumableRandomSampler

from marti.worlds.base_world import BaseWorld

@dataclass
class Agent:
    # One agent includes actor model (many workers), critic model, reference model, and vllm engines for PPO training
    # The agent can start training after making trajectory samples by itself or with other agents
    def __init__(self, 
                agent_id,
                agent_config, 
                vllm_engines: List,
                actor_model_group: Optional[PPORayActorGroup] = None,
                critic_model_group: Optional[PPORayActorGroup] = None,
                tokenizer = None,
                generate_kwargs = None,
                is_reasoning_model = False):
        self.agent_id = agent_id
        self.agent_config = agent_config
        self.vllm_engines = vllm_engines
        self.actor_model_group = actor_model_group
        self.critic_model_group = critic_model_group
        self.tokenizer = tokenizer
        self.generate_kwargs = generate_kwargs
        self.is_reasoning_model = is_reasoning_model

    def get_metadata(self):
        return {
            "agent_id": self.agent_id,
            "agent_role": self.agent_config.role,
            "pretrain": self.agent_config.pretrain,
            "llms": self.vllm_engines,
            "tokenizer": self.tokenizer,
            "generate_kwargs": self.generate_kwargs,
            "is_reasoning_model": self.is_reasoning_model
        }

    def save_actor_and_critic_model(self, folder="step-final"):
        if self.agent_config.is_tuning:
            save_path = os.path.join(self.agent_config.save_path, "checkpoint", folder, self.agent_id)
            ray.get(self.actor_model_group.async_save_model(save_path))
            if self.agent_config.critic_pretrain and self.agent_config.save_value_network:
                ray.get(self.critic_model_group.async_save_model(save_path))

            # save tokenizer
            self.tokenizer.save_pretrained(save_path)

@ray.remote
def generate_samples_remote(base_world, chunk_prompts, rank, world_size):
    return base_world.generate_samples(chunk_prompts, rank, world_size)


class BaseController(ABC):
    """
    Load prompt datasets
    Manage experience_maker
    Run global fit function (TODO: use MLFlow)
    Log status for each fit
        including various actors/critics
    """
    def __init__(self, strategy):
        super().__init__()
        self.strategy = strategy
        self.args = strategy.args
    
    def load_dataset(self, tokenizer=None):
        # prepare_datasets
        self.prepare_datasets(tokenizer)
        self.num_update_steps_per_episodes = (
            len(self.prompts_dataset) *
            self.args.n_samples_per_prompt // self.args.train_batch_size * self.args.max_epochs
        )
        max_steps = math.ceil(self.args.num_episodes *
                              self.num_update_steps_per_episodes)
        self._max_steps = max_steps

        # TODO: init logging, add MLFlow
        self._init_logging(self.strategy)

    def build(self):
        self._init_tok_kwargs(self.args, self.args.pretrain)
        self.load_dataset(self.tokenizer)
        # prepare agent includes many workers
        self.agent = self._init_agent()

        # create samples maker
        self.world = BaseWorld(
            strategy=self.strategy, agents=[self.agent.get_metadata()])

    def _init_tok_kwargs(self, args, pretrain):
        self.tokenizer = get_tokenizer(
            pretrain, None, "left", self.strategy, use_fast=not self.strategy.args.disable_fast_tokenizer)

        self.generate_kwargs = {
            "do_sample": True,
            "max_new_tokens": args.generate_max_len,
            "max_length": args.max_len,
            "temperature": args.temperature,
            "top_p": args.top_p,
            "pad_token_id": self.tokenizer.pad_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
        }

    def _init_agent(self):
        args = self.args
        strategy = self.strategy

        # init vllm / actor /critic /ref /reward model
        # if colocated, create placement group for actor and ref model explicitly.
        pg = None
        if args.colocate_actor_ref or args.colocate_all_models:
            assert (
                args.actor_num_nodes == args.ref_num_nodes and args.actor_num_gpus_per_node == args.ref_num_gpus_per_node
            ), f"num_nodes and num_gpus_per_node must be the same when colocate actor and ref model."

            bundles = [{"GPU": 1, "CPU": 1} for _ in range(args.actor_num_nodes * args.actor_num_gpus_per_node)]
            pg = placement_group(bundles, strategy="PACK")

            ray.get(pg.ready())


        # init vLLM engine for text generation
        vllm_engines = None
        if args.vllm_num_engines is not None and args.vllm_num_engines > 0:
            max_len = args.max_len if args.max_len else args.prompt_max_len + args.generate_max_len
            if args.colocate_all_models:
                assert (
                    args.actor_num_nodes * args.actor_num_gpus_per_node
                    == args.vllm_num_engines * args.vllm_tensor_parallel_size
                ), (
                    f"actor_num_nodes * actor_num_gpus_per_node must be equal to "
                    f"vllm_num_engines * vllm_tensor_parallel_size, got {args.actor_num_nodes * args.actor_num_gpus_per_node} "
                    f"and {args.vllm_num_engines * args.vllm_tensor_parallel_size}"
                )

            vllm_engines = create_vllm_engines(
                args.vllm_num_engines,
                args.vllm_tensor_parallel_size,
                args.pretrain,
                args.seed,
                args.enable_prefix_caching,
                args.enforce_eager,
                max_len,
                pg,
                args.vllm_gpu_memory_utilization,
                args.vllm_enable_sleep,
            )

        actor_model = PPORayActorGroup(
            args.actor_num_nodes,
            args.actor_num_gpus_per_node,
            ActorModelRayActor,
            pg=pg,
            num_gpus_per_actor=0.2 if pg else 1,
        )

        # if args.init_kl_coef <= 0:
        #     ref_model = None
        # else:
        ref_model = PPORayActorGroup(
            args.ref_num_nodes,
            args.ref_num_gpus_per_node,
            ReferenceModelRayActor,
            pg=pg,
            num_gpus_per_actor=0.2 if pg else 1,
        )

        if not args.colocate_all_models:
            pg = None

        # if colocated, create placement group for critic and reward model explicitly.
        if args.critic_pretrain and args.colocate_critic_reward:
            assert (
                args.critic_num_nodes == args.reward_num_nodes
                and args.critic_num_gpus_per_node == args.reward_num_gpus_per_node
            ), f"num_nodes and num_gpus_per_node must be the same when colocate critic and reward model."

            bundles = [{"GPU": 1, "CPU": 1} for _ in range(args.critic_num_nodes * args.critic_num_gpus_per_node)]
            pg = placement_group(bundles, strategy="PACK")
            ray.get(pg.ready())

        if args.critic_pretrain:
            critic_model = PPORayActorGroup(
                args.critic_num_nodes,
                args.critic_num_gpus_per_node,
                CriticModelRayActor,
                pg=pg,
                num_gpus_per_actor=0.2 if pg else 1,
            )
        else:
            critic_model = None

        # multiple reward models
        if args.reward_pretrain is not None:
            reward_pretrains = args.reward_pretrain.split(",")
            reward_models = []
            for _ in reward_pretrains:
                reward_models.append(
                    PPORayActorGroup(
                        args.reward_num_nodes,
                        args.reward_num_gpus_per_node,
                        RewardModelRayActor,
                        pg=pg,
                        num_gpus_per_actor=0.2 if pg else 1,
                    )
                )
        else:
            reward_models = None

        if ref_model is not None:
            # init reference/reward/actor model
            refs = ref_model.async_init_model_from_pretrained(
                strategy, args.pretrain)
            ray.get(refs)

        eos_token_ids = self.args.eos_token_id if self.args.eos_token_id is not None else self.tokenizer.eos_token_id
        refs = actor_model.async_init_model_from_pretrained(
            strategy, args.pretrain, self._max_steps, "actor", eos_token_ids)
        ray.get(refs)

        if args.reward_pretrain is not None:
            for reward_model, reward_pretrain in zip(reward_models, reward_pretrains):
                refs = reward_model.async_init_model_from_pretrained(
                    strategy, reward_pretrain)
                ray.get(refs)

        if args.critic_pretrain:
            # critic scheduler initialization depends on max_step, so we have to init critic after actor
            # TODO: use first reward model as critic model
            refs.extend(critic_model.async_init_model_from_pretrained(
                strategy, args.critic_pretrain, self._max_steps))
            ray.get(refs)

        # init actor and critic mdoel
        refs = actor_model.async_init_actor_trainer(
            critic_model, ref_model, reward_models, args.remote_rm_url, vllm_engines=vllm_engines
        )
        ray.get(refs)

        agent = Agent(
            agent_id="agent",
            agent_config=args,
            actor_model_group=actor_model,
            critic_model_group=critic_model,
            vllm_engines=vllm_engines,
            tokenizer=self.tokenizer,
            generate_kwargs=self.generate_kwargs,
            is_reasoning_model=args.is_reasoning_model
        )

        return agent

    def _init_logging(self, strategy):
        # wandb/tensorboard setting
        self._wandb = None
        self._tensorboard = None
        if self.strategy.args.use_wandb:
            import wandb

            from omegaconf import OmegaConf

            config_dict = OmegaConf.to_container(strategy.args, resolve=True, enum_to_str=True)

            self._wandb = wandb
            if not wandb.api.api_key:
                wandb.login(key=strategy.args.wandb_key)

            wandb.init(
                mode=strategy.args.wandb_mode,
                dir=strategy.args.wandb_path,
                entity=strategy.args.wandb_org,
                project=strategy.args.wandb_project,
                group=strategy.args.wandb_group,
                name=strategy.args.wandb_run_name,
                config=config_dict,
                reinit=True,
            )

            wandb.define_metric("train/global_step")
            wandb.define_metric(
                "train/*", step_metric="train/global_step", step_sync=True)

            # Try to resolve the issue of wandb step sync
            wandb.define_metric("eval/global_step")
            wandb.define_metric(
                "eval/*", step_metric="eval/global_step", step_sync=True)

        # Initialize TensorBoard writer if wandb is not available
        if self.strategy.args.use_tensorboard and self._wandb is None:
            from torch.utils.tensorboard import SummaryWriter

            os.makedirs(self.strategy.args.use_tensorboard, exist_ok=True)
            log_dir = os.path.join(
                self.strategy.args.use_tensorboard, strategy.args.wandb_run_name)
            self._tensorboard = SummaryWriter(log_dir=log_dir)

    def prepare_datasets(self, tokenizer):
        strategy = self.strategy
        args = self.strategy.args

        # prepare datasets
        prompts_data, prompts_dataset_eval = blending_datasets(
            args.prompt_data,
            args.prompt_data_probs,
            strategy,
            args.seed,
            max_count=args.max_samples,
            return_eval=True,
            train_split=args.prompt_split,
        )
        prompts_data = prompts_data.select(
            range(min(args.max_samples, len(prompts_data))))
        self.prompts_dataset = PromptDatasetWithLabel(
            prompts_data, tokenizer, strategy, input_template=args.input_template, add_prompt_suffix=args.add_prompt_suffix
        )
        self.prompts_dataset_eval = PromptDatasetWithLabel(
            prompts_dataset_eval, tokenizer, strategy, input_template=args.input_template, add_prompt_suffix=args.add_prompt_suffix
        )

        if args.extra_eval_tasks:
            self.prompts_dataset_extra_eval_tasks = {}
            extra_eval_tasks = args.extra_eval_tasks
            for task in extra_eval_tasks:
                prompts_data_extra_eval_task = load_dataset("json", data_files=os.path.join(args.extra_eval_dir, f"{task}.json"))["train"]
                self.prompts_dataset_extra_eval_tasks[task] = PromptDatasetWithLabel(
                    prompts_data_extra_eval_task, tokenizer, strategy, input_template=args.input_template, add_prompt_suffix=args.add_prompt_suffix
                )

        sampler = ResumableRandomSampler(
            data_source=self.prompts_dataset,
            batch_size=args.rollout_batch_size,
            drop_last=True,
            shuffle=True,
            seed=args.seed
        )
        self.prompts_dataloader = strategy.setup_dataloader(
            self.prompts_dataset, args.rollout_batch_size, True, True,
            sampler=sampler
        )

    def load_checkpoint_steps(self):
        args = self.args
        num_update_steps_per_episodes = (
            len(self.prompts_dataset) * args.n_samples_per_prompt // args.train_batch_size * args.max_epochs
        )
        consumed_samples = 0
        ckpt_path = os.path.join(args.ckpt_path, "_actor")
        if args.load_checkpoint and os.path.exists(ckpt_path):
            states = torch.load(os.path.join(ckpt_path, "step_states.bin"))
            consumed_samples = states["consumed_samples"]
            print(f"Loaded the checkpoint: {ckpt_path}, consumed_samples: {consumed_samples}")

        return num_update_steps_per_episodes, consumed_samples

    def run(self):
        # update steps if load checkpoints
        num_update_steps_per_episodes, consumed_samples = self.load_checkpoint_steps()
        
        # start fitting
        self.fit(
            consumed_samples=consumed_samples,
            num_update_steps_per_episodes=num_update_steps_per_episodes
        )
        
        # save actor and critic workers in agent
        self.agent.save_actor_and_critic_model(self.args, self.tokenizer)

    def clean_template(self, prompt):
        """
        clean the template
        '<|im_start|>system\nPlease reason step by step, and put your final answer within \\boxed{}.<|im_end|>\n<|im_start|>user\nhow are you?<|im_end|>\n<|im_start|>assistant\n'
        
        '<｜begin▁of▁sentence｜><｜User｜>how are you?<｜Assistant｜>'
        """
        if "<|im_start|>" in prompt:
            prompt = prompt.split("<|im_start|>user\n")[-1].split("<|im_end|>")[0]
        elif "<｜begin▁of▁sentence｜>" in prompt:
            prompt = prompt.split("<｜begin▁of▁sentence｜><｜User｜>")[-1].split("<｜Assistant｜>")[0]
        return prompt

    def build_sft_sample_list(self, data):
        """
        Input: {
        "sample": samples_list,
        "prompt": all_prompts,
        "output": all_pred_outputs,
        "label": all_pred_labels
        }
        
        Output: List[dict]
            {input_key: "", label_key:""}
        """
        all_prompts = data["prompt"]
        all_outputs = data["output"]
        all_labels = data["label"]
        assert len(all_prompts) == len(all_outputs) == len(all_labels)
        n_samples_per_prompt = self.args.n_samples_per_prompt
        sample_list = []
        for idx in range(0, len(all_prompts), n_samples_per_prompt):
            tmp_sample = []
            for i in range(n_samples_per_prompt):
                prompt = all_prompts[idx + i]
                output = all_outputs[idx + i]
                label = all_labels[idx + i]
                if label == 1:
                    prompt = self.clean_template(prompt)
                    tmp_sample.append({
                        self.args.input_key: prompt,
                        self.args.output_key: output,
                    })
            if len(tmp_sample) <= n_samples_per_prompt // 2:
                # sample_list.append(random.choice(tmp_sample))
                sample_list.extend(tmp_sample)
        return sample_list

    def build_sft_dataset(self, data_list):
        pretrain_data = Dataset.from_list(data_list)
        pretrain_max_len = self.strategy.args.max_len if self.strategy.args.max_len else self.strategy.args.prompt_max_len + \
                self.strategy.args.generate_max_len
        pretrain_dataset = SFTDataset(
            pretrain_data,
            self.tokenizer,
            pretrain_max_len,
            self.strategy,
            pretrain_mode=False,
            num_processors=1
        )
        assert len(pretrain_dataset) > 0, f"{len(pretrain_dataset)} samples are generated."
        return pretrain_dataset

    def generate_shared_samples(self, rand_prompts):
        world_size = self.agent.actor_model_group.world_size

        any_key = next(iter(rand_prompts.keys()))
        length = len(rand_prompts[any_key])
        chunk_size = (length + world_size - 1) // world_size
        chunked = [dict() for _ in range(world_size)]
        for key, data_list in rand_prompts.items():
            for i in range(world_size):
                start_idx = i * chunk_size
                end_idx = min(start_idx + chunk_size, length)
                sub_slice = data_list[start_idx:end_idx]
                chunked[i][key] = sub_slice
        
        all_refs = []
        for rank in range(world_size):
            samples_ref = generate_samples_remote.remote(self.world, chunked[rank], rank, world_size)
            all_refs.append(samples_ref)
        all_results = ray.get(all_refs)

        shared_data_refs = [None for _ in range(world_size)]
        sft_samples = []

        # Ensure all workers have the same number of samples, avoid stucking
        # Especially when we filter samples by rewards
        # TODO: Maybe we should decrease train_batch_size (acculate_grads_every_n_steps)
        # Reference: https://github.com/OpenRLHF/OpenRLHF/issues/717
        if self.args.filter_samples_by_reward:
            min_num_samples = min([len(x["sample"]) for x in all_results])
            num_full_batches = (min_num_samples * self.args.micro_rollout_batch_size) // self.args.train_batch_size
            num_elements_to_keep = (num_full_batches * self.args.train_batch_size) // self.args.micro_rollout_batch_size
        else:
            num_elements_to_keep = -1

        # create samples for each actor worker
        for rank in range(world_size):

            shared_data = all_results[rank]["sample"]
            if self.args.filter_samples_by_reward:
                shared_data = shared_data[:num_elements_to_keep]

            if self.args.training_mode in ["sft", "both", "mix"]:
                sft_samples.extend(self.build_sft_sample_list(shared_data))

            shared_ref = ray.put(shared_data)
            shared_data_refs[rank] = shared_ref
        
        if len(sft_samples) > 0:
            for sample in random.sample(sft_samples, 3):
                print(sample)
                print("="*20)
            return shared_data_refs, ray.put(self.build_sft_dataset(sft_samples)), num_elements_to_keep
        else:
            return shared_data_refs, None, num_elements_to_keep

    def step(self, rand_prompts, episode, steps, pbar):
        if self.args.vllm_enable_sleep:
            from marti.models.vllm.engine import batch_vllm_engine_call
            batch_vllm_engine_call(self.agent.vllm_engines, "wake_up")

        # make shared samples refs
        shared_data_refs, sft_dataset, num_elements_to_keep = self.generate_shared_samples(
            rand_prompts=rand_prompts)
        if self.args.vllm_enable_sleep:
            from marti.models.vllm.engine import batch_vllm_engine_call
            batch_vllm_engine_call(self.agent.vllm_engines, "sleep")

        # if there is not enough samples, then we generate samples again
        if self.args.filter_samples_by_reward and num_elements_to_keep == 0:
            return None

        # start training actor models (workers)
        refs = self.agent.actor_model_group.async_fit_actor_model(
            steps, shared_data_refs, sft_dataset)
        results = ray.get(refs)

        # find result from rank 0
        for result in results:
            logs_dict, is_rank_0, perf_stats = result["status"], result["is_rank_0"], result["perf_stats"]
            if is_rank_0:
                break

        return [logs_dict, perf_stats]

    def fit(self,
            consumed_samples=0,
            num_update_steps_per_episodes=1,
        ) -> None:
        args = self.args
        
        num_rollouts_per_episodes = (
            num_update_steps_per_episodes
            * args.train_batch_size
            // args.max_epochs
            // args.rollout_batch_size
            // args.n_samples_per_prompt
        )

        # Restore step and start_epoch
        steps = consumed_samples // args.rollout_batch_size + 1
        start_episode = consumed_samples // args.rollout_batch_size // num_rollouts_per_episodes
        consumed_samples = consumed_samples % (
            num_rollouts_per_episodes * args.rollout_batch_size)

        # Eval before training
        self.save_logs(args, 0, self.evaluate_tasks(0), None)

        for episode in range(start_episode, args.num_episodes):
            if isinstance(self.prompts_dataloader.sampler, ResumableRandomSampler):
                self.prompts_dataloader.sampler.set_epoch(
                    episode, consumed_samples=0 #if episode > start_episode else consumed_samples
                )
            pbar = tqdm(
                range(self.prompts_dataloader.__len__()),
                desc=f"Episode [{episode + 1}/{args.num_episodes}]"
            )

            # pass generated samples to each actor worker
            for rand_prompts in self.prompts_dataloader:
                
                step_result = self.step(rand_prompts, episode, steps, pbar)
                if step_result is None:
                    continue

                logs_dict, perf_stats = step_result

                if steps % args.eval_steps == 0:
                    logs_dict.update(self.evaluate_tasks(steps))

                if steps % args.save_steps == 0:
                    folder = f"step-{steps}"
                    for agent in self.agent_list:
                        agent.save_actor_and_critic_model(folder)
                        if self.args.shared_agents:
                            break

                self.save_logs(args, steps, logs_dict, perf_stats)

                pbar.set_postfix(logs_dict)
                pbar.update()
                steps = steps + 1

        if self._wandb is not None:
            self._wandb.finish()
        if self._tensorboard is not None:
            self._tensorboard.close()

    def list_of_dicts_to_dict_of_lists(self, list_of_dicts):
        result = defaultdict(list)
        for d in list_of_dicts:
            for key, value in d.items():
                result[key].append(value)
        return dict(result)

    def evaluate_samples(self, steps, eval_dataset, task="default"):
        data_list = eval_dataset.get_all_prompts()
        all_prompts = self.list_of_dicts_to_dict_of_lists(data_list)
        results = self.world.evaluate_samples(all_prompts)
        output_path = os.path.join(self.args.save_path, "evaluation", f"step-{steps}")
        if not os.path.exists(output_path):
            os.makedirs(output_path)
        srsly.write_json(os.path.join(output_path, f"{task}.json"), results)
        return results["accuracy"]

    def set_agent_vllm_engine(self, command):
        if self.args.vllm_enable_sleep:
            from marti.models.vllm.engine import batch_vllm_engine_call
            batch_vllm_engine_call(self.agent.vllm_engines, command)

    def evaluate_tasks(self, steps):
        self.set_agent_vllm_engine("wake_up")
        acc_dict = {"eval/overall/score": self.evaluate_samples(steps, self.prompts_dataset_eval, "overall")}
        if self.args.extra_eval_tasks:
            for task in self.prompts_dataset_extra_eval_tasks:
                acc_dict[f"eval/{task}/score"] = self.evaluate_samples(steps, self.prompts_dataset_extra_eval_tasks[task], task)
        self.set_agent_vllm_engine("sleep")
        return acc_dict

    def save_logs(self, args, global_step, logs_dict, perf_stats):
        if global_step % args.logging_steps == 0:
            # wandb
            if self._wandb is not None:
                # fix the logging of evaluation results
                logs = {}
                for k, v in logs_dict.items():
                    if "eval/" in k:
                        logs["eval/global_step"] = global_step
                        logs[k] = v
                    else:
                        logs["train/global_step"] = global_step
                        logs[f"train/{k}"] = v
                if perf_stats is not None:
                    logs.update({f"perf/experience_maker/{k}": v for k, v in perf_stats.items()})
                self._wandb.log(logs)
            # TensorBoard
            elif self._tensorboard is not None:
                for k, v in logs_dict.items():
                    # save eval logs to eval folder
                    if len(k.split("/")) == 2:
                        split, k = k.split("/")
                    else:
                        split = "train"
                    self._tensorboard.add_scalar(
                        f"{split}/{k}", v, global_step)
                if perf_stats is not None:
                    for k, v in perf_stats.items():
                        self._tensorboard.add_scalar(f"perf/experience_maker/{k}", v, global_step)
